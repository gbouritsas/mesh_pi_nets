{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from lib import mesh_sampling\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "from facemesh import FaceData\n",
    "import time\n",
    "import pickle\n",
    "import trimesh\n",
    "\n",
    "try:\n",
    "    import psbody.mesh\n",
    "    found = True\n",
    "except ImportError:\n",
    "    found = False\n",
    "if found:\n",
    "    from psbody.mesh import Mesh, MeshViewer, MeshViewers\n",
    "\n",
    "from autoencoder_dataset import autoencoder_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from spiral_utils import get_adj_trigs, generate_spirals\n",
    "from models import SpiralAutoencoder, SpiralAutoencoder_extra_conv\n",
    "\n",
    "from train_funcs import train_autoencoder, train_autoencoder_dataloader\n",
    "\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "meshpackage = 'trimesh'\n",
    "root_dir = '/data/gb318/datasets/'\n",
    "\n",
    "name = 'sliced'\n",
    "dataset = 'COMA'    \n",
    "\n",
    "GPU = True\n",
    "device_idx = 3\n",
    "torch.cuda.get_device_name(device_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "generative_model = 'autoencoder'\n",
    "dilation_flag = False\n",
    "hardcode_down_ref = False\n",
    "downsample_method = 'COMA_downsample' # choose'COMA_downsample' or 'meshlab_downsample'\n",
    "downsample_config = ''\n",
    "\n",
    "if dataset == 'COMA':\n",
    "    reference_mesh_file = os.path.join(root_dir, dataset,'preprocessed/templates/template.obj')\n",
    "    downsample_directory = os.path.join(root_dir, dataset,'preprocessed/templates',downsample_method,downsample_config)\n",
    "    ds_factors = [4, 4, 4, 4]\n",
    "    step_sizes = [1, 1, 1, 1, 1]\n",
    "    filter_sizes_enc = [[3, 16, 16, 16, 32],[[],[],[],[],[]]]\n",
    "    filter_sizes_dec = [[32, 32, 16, 16, 3],[[],[],[],[],[]]]\n",
    "    if dilation_flag:\n",
    "        dilation=[2, 2, 2, 1, 1] \n",
    "    else:\n",
    "        dilation = None\n",
    "\n",
    "args = {'generative_model': generative_model,\n",
    "        'name': name, 'data': os.path.join(root_dir, dataset, 'preprocessed',name),\n",
    "        'results_folder':  os.path.join(root_dir, dataset,'results/higher_order_'+ generative_model,\\\n",
    "                                        downsample_method, downsample_config,'3nd_order_full_linear'),\n",
    "        'reference_mesh_file':reference_mesh_file, 'downsample_directory': downsample_directory,\n",
    "        'checkpoint_file': 'checkpoint',\n",
    "        'seed':2, 'loss':'l1',\n",
    "        'batch_size':16, 'num_epochs':300, 'eval_frequency':200, 'num_workers': 4,\n",
    "        'filter_sizes_enc': filter_sizes_enc, 'filter_sizes_dec': filter_sizes_dec,\n",
    "        'nz':16, \n",
    "        'ds_factors': ds_factors, 'step_sizes' : step_sizes, 'dilation': dilation,\n",
    "        'injection': True, 'residual': True, \n",
    "        \n",
    "        'lr':1e-3, \n",
    "        'regularization': 5e-5,         \n",
    "        'scheduler': True, 'decay_rate': 0.99,'decay_steps':1,  \n",
    "        'resume': False,\n",
    "        \n",
    "        'mode':'train', 'shuffle': True, 'nVal': 100, 'normalization': True}\n",
    "\n",
    "if generative_model == 'autoencoder':\n",
    "    args['results_folder'] = os.path.join(args['results_folder'],\\\n",
    "                                          'latent_'+str(args['nz']))\n",
    "    \n",
    "if not os.path.exists(os.path.join(args['results_folder'])):\n",
    "    os.makedirs(os.path.join(args['results_folder']))\n",
    "\n",
    "summary_path = os.path.join(args['results_folder'],'summaries',args['name'])\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)  \n",
    "    \n",
    "checkpoint_path = os.path.join(args['results_folder'],'checkpoints', args['name'])\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "    \n",
    "samples_path = os.path.join(args['results_folder'],'samples', args['name'])\n",
    "if not os.path.exists(samples_path):\n",
    "    os.makedirs(samples_path)\n",
    "    \n",
    "prediction_path = os.path.join(args['results_folder'],'predictions', args['name'])\n",
    "if not os.path.exists(prediction_path):\n",
    "    os.makedirs(prediction_path)\n",
    "\n",
    "if not os.path.exists(downsample_directory):\n",
    "    os.makedirs(downsample_directory)\n",
    "\n",
    "if hardcode_down_ref:\n",
    "    if dataset == 'COMA' and downsample_method == 'COMA_downsample':\n",
    "        reference_points = [[3567,4051,4597],\n",
    "                            [1010,1081,1170],\n",
    "                            [256, 276, 295],\n",
    "                            [11, 69, 74],\n",
    "                            [17, 17, 17]]\n",
    "    elif dataset == 'COMA' and downsample_method == 'meshlab_downsample' and downsample_config == 'preserve_topology=True_preserve_boundary=False':\n",
    "        reference_points = [[3567, 4051, 4597],\n",
    "                             [1105, 1214, 1241],\n",
    "                             [289, 310, 318],\n",
    "                             [70, 80, 85],\n",
    "                             [2, 19, 24]]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "else:\n",
    "    if dataset == 'COMA':\n",
    "        reference_points = [[3567,4051,4597]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data .. \n",
      "Loading Transform Matrices ..\n",
      "Calculating reference points for downsampled versions..\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(args['seed'])\n",
    "print(\"Loading data .. \")\n",
    "if not os.path.exists(args['data']+'/mean.npy') or not os.path.exists(args['data']+'/std.npy'):\n",
    "    facedata = FaceData(nVal=args['nVal'], train_file=args['data']+'/train.npy',\n",
    "                             test_file=args['data']+'/test.npy', reference_mesh_file=args['reference_mesh_file'],\n",
    "                             pca_n_comp=args['nz'], normalization = args['normalization'],\\\n",
    "                             meshpackage = meshpackage, load_flag = True)\n",
    "    np.save(args['data']+'/mean.npy', facedata.mean)\n",
    "    np.save(args['data']+'/std.npy', facedata.std)\n",
    "else:\n",
    "    facedata = FaceData(nVal=args['nVal'], train_file=args['data']+'/train.npy',\\\n",
    "                        test_file=args['data']+'/test.npy', reference_mesh_file=args['reference_mesh_file'],\\\n",
    "                        pca_n_comp=args['nz'], normalization = args['normalization'],\\\n",
    "                        meshpackage = meshpackage, load_flag = False)\n",
    "    facedata.mean = np.load(args['data']+'/mean.npy')\n",
    "    facedata.std = np.load(args['data']+'/std.npy')\n",
    "    facedata.n_vertex = facedata.mean.shape[0]\n",
    "    facedata.n_features = facedata.mean.shape[1]\n",
    "\n",
    "if not os.path.exists(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl')):\n",
    "    if facedata.meshpackage == 'trimesh':\n",
    "        raise NotImplementedError\n",
    "    print(\"Generating Transform Matrices ..\")\n",
    "\n",
    "\n",
    "    if downsample_method == 'COMA_downsample':\n",
    "        M,A,D,U,F = mesh_sampling.generate_transform_matrices(facedata.reference_mesh, args['ds_factors'])\n",
    "    elif downsample_method == 'meshlab_downsample':\n",
    "        M,A,D,U,F = mesh_sampling.generate_transform_matrices_given_downsamples(facedata.reference_mesh,                                                                                args['downsample_directory'],                                                                                len(args['ds_factors']))\n",
    "    else:\n",
    "        raise NotImplementedError(downsample_method)\n",
    "        \n",
    "    with open(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl'), 'wb') as fp:\n",
    "        M_verts_faces = [(M[i].v, M[i].f) for i in range(len(M))]\n",
    "        pickle.dump({'M_verts_faces':M_verts_faces,'A':A,'D':D,'U':U,'F':F}, fp)\n",
    "else:\n",
    "    print(\"Loading Transform Matrices ..\")\n",
    "    with open(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl'), 'rb') as fp:\n",
    "        downsampling_matrices = pickle.load(fp,encoding = 'latin1')\n",
    "            \n",
    "    M_verts_faces = downsampling_matrices['M_verts_faces']\n",
    "    if facedata.meshpackage == 'mpi-mesh':\n",
    "        M = [Mesh(v=M_verts_faces[i][0], f=M_verts_faces[i][1]) for i in range(len(M_verts_faces))]\n",
    "    elif facedata.meshpackage == 'trimesh':\n",
    "        M = [trimesh.base.Trimesh(vertices=M_verts_faces[i][0], faces=M_verts_faces[i][1], process = False)             for i in range(len(M_verts_faces))]\n",
    "    A = downsampling_matrices['A']\n",
    "    D = downsampling_matrices['D']\n",
    "    U = downsampling_matrices['U']\n",
    "    F = downsampling_matrices['F']\n",
    "        \n",
    "\n",
    "if not hardcode_down_ref:\n",
    "    print(\"Calculating reference points for downsampled versions..\")\n",
    "    for i in range(len(args['ds_factors'])):\n",
    "        if facedata.meshpackage == 'mpi-mesh':\n",
    "            dist = euclidean_distances(M[i+1].v, M[0].v[reference_points[0]])\n",
    "        elif facedata.meshpackage == 'trimesh':\n",
    "            dist = euclidean_distances(M[i+1].vertices, M[0].vertices[reference_points[0]])\n",
    "        reference_points.append(np.argmin(dist,axis=0).tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spiral generation for hierarchy 0 (5023 vertices) finished\n",
      "spiral generation for hierarchy 1 (1256 vertices) finished\n",
      "spiral generation for hierarchy 2 (314 vertices) finished\n",
      "spiral generation for hierarchy 3 (79 vertices) finished\n",
      "spiral generation for hierarchy 4 (20 vertices) finished\n",
      "spiral sizes for hierarchy 0:  9\n",
      "spiral sizes for hierarchy 1:  9\n",
      "spiral sizes for hierarchy 2:  9\n",
      "spiral sizes for hierarchy 3:  9\n",
      "spiral sizes for hierarchy 4:  8\n"
     ]
    }
   ],
   "source": [
    "if facedata.meshpackage == 'mpi-mesh':\n",
    "    sizes = [x.v.shape[0] for x in M]\n",
    "elif facedata.meshpackage == 'trimesh':\n",
    "    sizes = [x.vertices.shape[0] for x in M]\n",
    "Adj, Trigs = get_adj_trigs(A, F, facedata.reference_mesh, meshpackage = facedata.meshpackage)\n",
    "\n",
    "spirals_np, spiral_sizes,spirals = generate_spirals(args['step_sizes'], M, Adj, Trigs, \\\n",
    "                                                    reference_points = reference_points, \\\n",
    "                                                    dilation = args['dilation'], random = False, \\\n",
    "                                                    meshpackage = facedata.meshpackage, counter_clockwise = True)\n",
    "\n",
    "bU = []\n",
    "bD = []\n",
    "for i in range(len(D)):\n",
    "    d = np.zeros((1,D[i].shape[0]+1,D[i].shape[1]+1))\n",
    "    u = np.zeros((1,U[i].shape[0]+1,U[i].shape[1]+1))\n",
    "    d[0,:-1,:-1] = D[i].todense()\n",
    "    u[0,:-1,:-1] = U[i].todense()\n",
    "    d[0,-1,-1] = 1\n",
    "    u[0,-1,-1] = 1\n",
    "    bD.append(d)\n",
    "    bU.append(u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "# pytorch stuff\n",
    "\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda:\"+str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "tspirals = [torch.from_numpy(s).long().to(device) for s in spirals_np]\n",
    "tD = [torch.from_numpy(s).float().to(device) for s in bD]\n",
    "tU = [torch.from_numpy(s).float().to(device) for s in bU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model, optimizer, and loss function\n",
    "\n",
    "dataset_train = autoencoder_dataset(root_dir = args['data'], points_dataset = 'train',\n",
    "                                           facedata = facedata,\n",
    "                                           normalization = args['normalization'])\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=args['batch_size'],\\\n",
    "                                     shuffle = args['shuffle'], num_workers = args['num_workers'])\n",
    "\n",
    "dataset_val = autoencoder_dataset(root_dir = args['data'], points_dataset = 'val', \n",
    "                                         facedata = facedata,\n",
    "                                         normalization = args['normalization'])\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=args['batch_size'],\\\n",
    "                                     shuffle = False, num_workers = args['num_workers'])\n",
    "\n",
    "\n",
    "dataset_test = autoencoder_dataset(root_dir = args['data'], points_dataset = 'test',\n",
    "                                          facedata = facedata,\n",
    "                                          normalization = args['normalization'])\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=args['batch_size'],\\\n",
    "                                     shuffle = False, num_workers = args['num_workers'])\n",
    "\n",
    "\n",
    "\n",
    "if 'autoencoder' in args['generative_model']:\n",
    "        model = SpiralAutoencoder_extra_conv(filters_enc = args['filter_sizes_enc'],   \n",
    "                                             filters_dec = args['filter_sizes_dec'],\n",
    "                                             latent_size=args['nz'],\n",
    "                                             sizes=sizes,\n",
    "                                             spiral_sizes=spiral_sizes,\n",
    "                                             spirals=tspirals,\n",
    "                                             D=tD, U=tU,device=device,\n",
    "                                             injection = args['injection'],\n",
    "                                             residual = args['residual']).to(device)\n",
    " \n",
    "    \n",
    "optim = torch.optim.Adam(model.parameters(),lr=args['lr'],weight_decay=args['regularization'])\n",
    "if args['scheduler']:\n",
    "    scheduler=torch.optim.lr_scheduler.StepLR(optim, args['decay_steps'],gamma=args['decay_rate'])\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "def loss_l1(outputs, targets):\n",
    "    L = torch.abs(outputs - targets).mean()\n",
    "    return L \n",
    "if args['loss']=='l1':\n",
    "    loss_fn = loss_l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters is: 734516\n",
      "SpiralAutoencoder_extra_conv(\n",
      "  (conv): ModuleList(\n",
      "    (0): SpiralConv(\n",
      "      (conva2): Linear(in_features=27, out_features=16, bias=False)\n",
      "      (convs2): Linear(in_features=27, out_features=16, bias=False)\n",
      "      (conva3): Linear(in_features=27, out_features=16, bias=False)\n",
      "      (convs3): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (normalizer3): BatchNorm1d(80384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (normalizer2): BatchNorm1d(80384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (1): SpiralConv(\n",
      "      (conva2): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (convs2): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (conva3): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (convs3): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (normalizer3): BatchNorm1d(20112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (normalizer2): BatchNorm1d(20112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (2): SpiralConv(\n",
      "      (conva2): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (convs2): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (conva3): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (convs3): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (normalizer3): BatchNorm1d(5040, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (normalizer2): BatchNorm1d(5040, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (3): SpiralConv(\n",
      "      (conva2): Linear(in_features=144, out_features=32, bias=False)\n",
      "      (convs2): Linear(in_features=144, out_features=32, bias=False)\n",
      "      (conva3): Linear(in_features=144, out_features=32, bias=False)\n",
      "      (convs3): Linear(in_features=288, out_features=32, bias=False)\n",
      "      (normalizer3): BatchNorm1d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (normalizer2): BatchNorm1d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (fc_latent_enc): Linear(in_features=672, out_features=16, bias=True)\n",
      "  (fc_latent_dec): Linear(in_features=16, out_features=672, bias=True)\n",
      "  (dconv): ModuleList(\n",
      "    (0): SpiralConv(\n",
      "      (conva2): Linear(in_features=288, out_features=32, bias=False)\n",
      "      (convs2): Linear(in_features=288, out_features=32, bias=False)\n",
      "      (conva3): Linear(in_features=288, out_features=32, bias=False)\n",
      "      (convs3): Linear(in_features=288, out_features=32, bias=False)\n",
      "      (normalizer3): BatchNorm1d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (normalizer2): BatchNorm1d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (1): SpiralConv(\n",
      "      (conva2): Linear(in_features=288, out_features=16, bias=False)\n",
      "      (convs2): Linear(in_features=288, out_features=16, bias=False)\n",
      "      (conva3): Linear(in_features=288, out_features=16, bias=False)\n",
      "      (convs3): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (normalizer3): BatchNorm1d(5040, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (normalizer2): BatchNorm1d(5040, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (2): SpiralConv(\n",
      "      (conva2): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (convs2): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (conva3): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (convs3): Linear(in_features=144, out_features=16, bias=False)\n",
      "      (normalizer3): BatchNorm1d(20112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (normalizer2): BatchNorm1d(20112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (3): SpiralConv(\n",
      "      (conva2): Linear(in_features=144, out_features=3, bias=False)\n",
      "      (convs2): Linear(in_features=144, out_features=3, bias=False)\n",
      "      (conva3): Linear(in_features=144, out_features=3, bias=False)\n",
      "      (convs3): Linear(in_features=27, out_features=3, bias=False)\n",
      "      (normalizer3): BatchNorm1d(15072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (normalizer2): BatchNorm1d(15072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters is: {}\".format(params)) \n",
    "print(model)\n",
    "# print(M[4].v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [02:20<00:00,  8.12it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 17.25it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | tr 0.4293937407944523 | val 3.487122657647015e+31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [02:23<00:00,  7.98it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 16.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | tr 0.21870985341680657 | val 0.1974928045272827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [02:22<00:00,  8.01it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 17.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 | tr 0.1726651283437165 | val 0.17701055943965913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:56<00:00,  9.86it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 16.53it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 | tr 0.15639347170517479 | val 0.16433643519878388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.89it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 19.02it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 | tr 0.14707886354619937 | val 0.17740942478179933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.88it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 20.02it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 | tr 0.14138107103807432 | val 0.14390786468982697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:21<00:00, 13.97it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 18.68it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 | tr 0.13781344367040171 | val 0.1413199496269226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.81it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 18.08it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 | tr 0.1344609313996607 | val 0.1258178347349167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.91it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 18.36it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 | tr 0.13218387401546575 | val 0.1456007891893387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:23<00:00, 13.79it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 20.55it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 | tr 0.1307812902581129 | val 0.13972842812538147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:21<00:00, 13.98it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 18.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | tr 0.12861958766143047 | val 0.1203018894791603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.84it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 19.46it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | tr 0.12728521143628574 | val 0.1233891212940216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.88it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 18.64it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | tr 0.12665921754844195 | val 0.13212804973125458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:23<00:00, 13.75it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 18.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | tr 0.12500657794858275 | val 0.12520072340965271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:23<00:00, 13.69it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 20.12it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | tr 0.12391286694324964 | val 0.11926580250263213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.81it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 18.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 | tr 0.12309604783039291 | val 0.12951051354408263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:21<00:00, 13.97it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 21.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | tr 0.12252782424665233 | val 0.11881381273269653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.85it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 19.65it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 | tr 0.12209893058199327 | val 0.1198644608259201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.81it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 20.14it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 | tr 0.12099351480037764 | val 0.11826288789510726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:21<00:00, 14.00it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 19.55it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | tr 0.1208671858470654 | val 0.12460216701030731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.83it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 15.63it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 | tr 0.11980148507554186 | val 0.1143808501958847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.87it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 19.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 | tr 0.11938062268318193 | val 0.11511014193296433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:23<00:00, 13.78it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 19.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 | tr 0.1186289969434056 | val 0.11608388245105744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.88it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 18.61it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 | tr 0.11851554822513323 | val 0.11177601873874664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.84it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 18.32it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 | tr 0.11792638739446393 | val 0.11924126625061035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:24<00:00, 13.60it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 20.48it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 | tr 0.11709589752430412 | val 0.11534961402416229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:23<00:00, 13.78it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 20.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 | tr 0.11683058642621838 | val 0.1095537132024765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.95it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 19.20it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 | tr 0.11671207360669039 | val 0.11480250418186187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.95it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 19.15it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 | tr 0.1163645397935804 | val 0.11332920491695404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:22<00:00, 13.91it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 | tr 0.11589044069221949 | val 0.10245580196380616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [01:23<00:00, 13.73it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 21.40it/s]\n",
      "  0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | tr 0.11519356639641345 | val 0.10347730457782746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 787/1145 [00:57<00:25, 14.18it/s]"
     ]
    }
   ],
   "source": [
    "if args['mode'] == 'train':\n",
    "    writer = SummaryWriter(summary_path)\n",
    "    with open(os.path.join(args['results_folder'],'checkpoints', args['name'] +'_params.json'),'w') as fp:\n",
    "        saveparams = copy.deepcopy(args)\n",
    "        json.dump(saveparams, fp)\n",
    "        \n",
    "    if args['resume']:\n",
    "            print('loading checkpoint from file %s'%(os.path.join(checkpoint_path,args['checkpoint_file'])))\n",
    "            checkpoint_dict = torch.load(os.path.join(checkpoint_path,args['checkpoint_file']+'.pth.tar'),map_location=device)\n",
    "            start_epoch = checkpoint_dict['epoch'] + 1\n",
    "            model.load_state_dict(checkpoint_dict['autoencoder_state_dict'])\n",
    "            optim.load_state_dict(checkpoint_dict['optimizer_state_dict'])\n",
    "            scheduler.load_state_dict(checkpoint_dict['scheduler_state_dict'])\n",
    "            print('Resuming from epoch %s'%(str(start_epoch)))     \n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        \n",
    "    if args['generative_model'] == 'autoencoder':\n",
    "        train_autoencoder_dataloader(dataloader_train, dataloader_val,\n",
    "                          device, model, optim, loss_fn,\n",
    "                          bsize = args['batch_size'],\n",
    "                          start_epoch = start_epoch,\n",
    "                          n_epochs = args['num_epochs'],\n",
    "                          eval_freq = args['eval_frequency'],\n",
    "                          scheduler = scheduler,\n",
    "                          writer = writer,\n",
    "                          save_recons=True,\n",
    "                          facedata=facedata,\n",
    "                          metadata_dir=checkpoint_path, samples_dir=samples_path,\n",
    "                          checkpoint_path = args['checkpoint_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
